#!/usr/bin/env python3
"""
Latency Analysis: Different Service Architecture Approaches
"""
import time
import asyncio
import requests
import websockets
import json

class LatencyAnalyzer:
    def __init__(self):
        self.results = {}
    
    async def test_http_service_latency(self, service_url, payload):
        """Test HTTP service latency"""
        start_time = time.time()
        try:
            response = requests.post(service_url, json=payload, timeout=10)
            end_time = time.time()
            latency = (end_time - start_time) * 1000  # Convert to ms
            return {
                'success': True,
                'latency_ms': latency,
                'status_code': response.status_code
            }
        except Exception as e:
            end_time = time.time()
            return {
                'success': False,
                'latency_ms': (end_time - start_time) * 1000,
                'error': str(e)
            }
    
    async def test_websocket_persistent_latency(self, ws_url, payload):
        """Test WebSocket with persistent connection latency"""
        try:
            start_connect = time.time()
            async with websockets.connect(ws_url) as websocket:
                connect_time = time.time()
                connection_latency = (connect_time - start_connect) * 1000
                
                # Send message
                start_send = time.time()
                await websocket.send(json.dumps(payload))
                response = await websocket.recv()
                end_send = time.time()
                message_latency = (end_send - start_send) * 1000
                
                return {
                    'success': True,
                    'connection_latency_ms': connection_latency,
                    'message_latency_ms': message_latency,
                    'total_latency_ms': connection_latency + message_latency
                }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    async def test_websocket_connect_per_request_latency(self, ws_url, payload):
        """Test WebSocket with new connection per request latency"""
        start_time = time.time()
        try:
            async with websockets.connect(ws_url) as websocket:
                await websocket.send(json.dumps(payload))
                response = await websocket.recv()
                end_time = time.time()
                return {
                    'success': True,
                    'total_latency_ms': (end_time - start_time) * 1000
                }
        except Exception as e:
            end_time = time.time()
            return {
                'success': False,
                'latency_ms': (end_time - start_time) * 1000,
                'error': str(e)
            }
    
    def analyze_architecture_latency(self):
        """Analyze different architectural approaches"""
        print("üîç LATENCY ANALYSIS: Service Architecture Approaches")
        print("=" * 70)
        
        architectures = {
            "Pure HTTP REST": {
                "description": "Traditional HTTP services, new connection per request",
                "connection_overhead": "TCP + HTTP handshake (~2-5ms)",
                "warm_service_latency": "0ms (always warm)",
                "cold_start_penalty": "Model loading time (100-2000ms)",
                "pros": ["Simple", "Stateless", "HTTP caching", "Load balancer friendly"],
                "cons": ["Connection overhead per request", "No streaming"],
                "typical_latency": "5-20ms + processing time"
            },
            
            "WebSocket Persistent": {
                "description": "WebSocket connections stay open during conversation",
                "connection_overhead": "TCP + WS handshake once (~3-8ms)",
                "warm_service_latency": "0ms (connection pooled)",
                "cold_start_penalty": "Initial connection setup only",
                "pros": ["No per-request overhead", "Streaming support", "Real-time"],
                "cons": ["Connection management", "Scaling complexity", "Resource usage"],
                "typical_latency": "1-3ms + processing time"
            },
            
            "WebSocket Per-Request": {
                "description": "New WebSocket connection for each request",
                "connection_overhead": "TCP + WS handshake per request (~3-8ms)",
                "warm_service_latency": "Connection setup time",
                "cold_start_penalty": "Model loading + connection setup",
                "pros": ["Stateless like HTTP", "Streaming capable"],
                "cons": ["Worst of both worlds", "High overhead"],
                "typical_latency": "8-25ms + processing time"
            },
            
            "HYBRID (Recommended)": {
                "description": "HTTP services always warm + WebSocket for conversations",
                "connection_overhead": "HTTP: ~2-5ms, WS: ~3-8ms (per conversation)",
                "warm_service_latency": "HTTP: 2-5ms, WS: 1-3ms after connection",
                "cold_start_penalty": "Eliminated (services always warm)",
                "pros": ["Best of both", "Services always ready", "Streaming when needed"],
                "cons": ["Slightly more complex", "Dual endpoints"],
                "typical_latency": "HTTP: 5-15ms, WS: 1-3ms per message"
            }
        }
        
        for arch_name, details in architectures.items():
            print(f"\nüìê {arch_name}")
            print("-" * 50)
            print(f"Description: {details['description']}")
            print(f"Connection Overhead: {details['connection_overhead']}")
            print(f"Warm Latency: {details['warm_service_latency']}")
            print(f"Cold Start: {details['cold_start_penalty']}")
            print(f"Typical Latency: {details['typical_latency']}")
            print(f"Pros: {', '.join(details['pros'])}")
            print(f"Cons: {', '.join(details['cons'])}")
    
    def analyze_conversation_scenarios(self):
        """Analyze latency for different conversation scenarios"""
        print("\n\nüó£Ô∏è CONVERSATION SCENARIO ANALYSIS")
        print("=" * 70)
        
        scenarios = {
            "Single Query": {
                "description": "One-off question/answer",
                "http_overhead": "5-15ms (connection + request)",
                "ws_persistent_overhead": "3-8ms (initial) + 1-3ms (request)",
                "ws_per_request_overhead": "8-25ms (connection + request)",
                "hybrid_overhead": "5-15ms (HTTP is fine for single queries)",
                "recommendation": "HTTP (simpler for single queries)"
            },
            
            "Multi-turn Conversation": {
                "description": "5-10 exchanges back and forth",
                "http_overhead": "50-150ms total (5-15ms √ó 10 requests)",
                "ws_persistent_overhead": "8ms + 10-30ms total (3-8ms setup + 1-3ms √ó 10)",
                "ws_per_request_overhead": "80-250ms total (8-25ms √ó 10)",
                "hybrid_overhead": "8ms + 10-30ms (WS for conversation)",
                "recommendation": "WebSocket Persistent or Hybrid"
            },
            
            "Voice Conversation": {
                "description": "Real-time voice with streaming",
                "http_overhead": "Not suitable (no streaming)",
                "ws_persistent_overhead": "8ms setup + <1ms per chunk",
                "ws_per_request_overhead": "Terrible (connection per audio chunk)",
                "hybrid_overhead": "8ms setup + <1ms per chunk (WS for voice)",
                "recommendation": "WebSocket Persistent (Hybrid approach)"
            },
            
            "Background Tasks": {
                "description": "Periodic processing, health checks",
                "http_overhead": "5-15ms per check (perfect for this)",
                "ws_persistent_overhead": "Resource waste (persistent connection unused)",
                "ws_per_request_overhead": "8-25ms per check (wasteful)",
                "hybrid_overhead": "5-15ms per check (HTTP for background)",
                "recommendation": "HTTP (Hybrid approach)"
            }
        }
        
        for scenario_name, details in scenarios.items():
            print(f"\nüé¨ {scenario_name}")
            print("-" * 40)
            print(f"Description: {details['description']}")
            print(f"HTTP: {details['http_overhead']}")
            print(f"WS Persistent: {details['ws_persistent_overhead']}")
            print(f"WS Per-Request: {details['ws_per_request_overhead']}")
            print(f"Hybrid: {details['hybrid_overhead']}")
            print(f"üéØ Best: {details['recommendation']}")

def main():
    analyzer = LatencyAnalyzer()
    
    print("üöÄ LATENCY ANALYSIS FOR SERVICE ARCHITECTURES")
    print("=" * 70)
    print("Analyzing the performance implications of different approaches...\n")
    
    analyzer.analyze_architecture_latency()
    analyzer.analyze_conversation_scenarios()
    
    print("\n\nüíé FINAL RECOMMENDATION: HYBRID APPROACH")
    print("=" * 70)
    print("‚úÖ BEST LATENCY PROFILE:")
    print("  ‚Ä¢ Single queries: HTTP (~5-15ms) - Simple and fast")
    print("  ‚Ä¢ Multi-turn chats: WebSocket (~1-3ms per message) - Minimal overhead")
    print("  ‚Ä¢ Voice streaming: WebSocket (<1ms per chunk) - Real-time capable")
    print("  ‚Ä¢ Health checks: HTTP (~5-15ms) - No persistent connections needed")
    print("\n‚úÖ HYBRID ADVANTAGES:")
    print("  ‚Ä¢ Services always warm (no cold start penalty)")
    print("  ‚Ä¢ Choose optimal transport per use case")
    print("  ‚Ä¢ WebSocket only when needed (saves resources)")
    print("  ‚Ä¢ HTTP for simple operations (better debugging/monitoring)")
    print("\n‚ö° LATENCY ADDED BY HYBRID: 0ms")
    print("  ‚Ä¢ No additional latency - you choose the faster option per scenario")
    print("  ‚Ä¢ HTTP when appropriate, WebSocket when beneficial")
    print("  ‚Ä¢ Services stay warm regardless of transport method")

if __name__ == "__main__":
    main()
