#!/usr/bin/env python3
"""
GPU Detection and Optimization for Tortoise TTS
"""
import torch
import sys
import os

def check_gpu_availability():
    """Check GPU availability and configuration"""
    print("üîç GPU Detection for Tortoise TTS")
    print("=" * 50)
    
    # Check CUDA
    if torch.cuda.is_available():
        print("‚úÖ CUDA Available!")
        device_count = torch.cuda.device_count()
        print(f"üìä GPU Count: {device_count}")
        
        for i in range(device_count):
            device_name = torch.cuda.get_device_name(i)
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            
            print(f"\nüéÆ GPU {i}: {device_name}")
            print(f"   Memory: {memory_gb:.1f} GB")
            print(f"   Compute Capability: {props.major}.{props.minor}")
            print(f"   Multi-Processors: {props.multi_processor_count}")
            
            # Check memory availability
            if memory_gb >= 4:
                print(f"   ‚úÖ Suitable for Tortoise TTS")
            else:
                print(f"   ‚ö†Ô∏è May be limited for large models")
        
        # Test GPU with simple tensor
        try:
            test_tensor = torch.randn(1000, 1000).cuda()
            result = torch.matmul(test_tensor, test_tensor)
            print(f"\n‚úÖ GPU Test Passed - Ready for neural synthesis!")
            del test_tensor, result
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"\n‚ùå GPU Test Failed: {e}")
            
    elif torch.backends.mps.is_available():
        print("‚úÖ Apple Metal Performance Shaders (MPS) Available!")
        print("üçé Optimized for Apple Silicon (M1/M2/M3)")
        
        try:
            test_tensor = torch.randn(1000, 1000).to('mps')
            result = torch.matmul(test_tensor, test_tensor)
            print("‚úÖ MPS Test Passed - Ready for neural synthesis!")
            del test_tensor, result
        except Exception as e:
            print(f"‚ùå MPS Test Failed: {e}")
            
    else:
        print("‚ùå No GPU acceleration available")
        print("üíª Will use CPU - synthesis will be slower")
        
    # Check PyTorch version
    print(f"\nüì¶ PyTorch Version: {torch.__version__}")
    
    # Performance recommendations
    print(f"\nüöÄ Performance Recommendations:")
    if torch.cuda.is_available():
        print("   ‚Ä¢ Use 'ultra_fast' preset for real-time synthesis")
        print("   ‚Ä¢ GPU acceleration will provide 5-10x speedup") 
        print("   ‚Ä¢ Larger batch sizes possible with more GPU memory")
    else:
        print("   ‚Ä¢ Stick to 'ultra_fast' preset for best performance")
        print("   ‚Ä¢ Consider shorter text segments")
        print("   ‚Ä¢ CPU synthesis may take 2-5 minutes per clip")

if __name__ == "__main__":
    check_gpu_availability()
